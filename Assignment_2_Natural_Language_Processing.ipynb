{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.x"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5miUaSiZYEpL"},"source":["# Assignment 2: Pre-trained Word Embeddings on LSTM Text Classification (Transfer Learning)\n","- In an LSTM-based text classification model, compare\n","  1. randomly initialized Embedding\n","  2. pre-trained GloVe-initialized Embedding\n","- to determine whether pre-training method as a regularization mechanism."],"id":"5miUaSiZYEpL"},{"cell_type":"markdown","source":["## 0. Environment Check"],"metadata":{"id":"n_yHwLO6OII9"},"id":"n_yHwLO6OII9"},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"id":"aIQO-UZxOLKR"},"id":"aIQO-UZxOLKR","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRDioDwwYEpN"},"source":["## 1. Dataset: AG_NEWS\n","\n","We use the CSV version of the `AG_NEWS` dataset by downloading it directly.\n","\n","https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset\n","\n","- 4 classes (World, Sports, Business, Sci/Tech)\n","- Format: `label,title,text`"],"id":"cRDioDwwYEpN"},{"cell_type":"code","metadata":{"id":"E_y3wKpGYEpN"},"execution_count":null,"outputs":[],"source":["import os\n","\n","data_dir = \"data_ag_news\"\n","os.makedirs(data_dir, exist_ok=True)\n","\n","train_csv = os.path.join(data_dir, \"train.csv\")\n","test_csv = os.path.join(data_dir, \"test.csv\")\n","\n","if not os.path.exists(train_csv) or not os.path.exists(test_csv):\n","    # AG_NEWS CSV (widely used mirror)\n","    !wget -O $train_csv https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n","    !wget -O $test_csv  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\n","\n","print(\"Files:\", os.listdir(data_dir))"],"id":"E_y3wKpGYEpN"},{"cell_type":"markdown","metadata":{"id":"s1f7JyBvYEpO"},"source":["### 1-1. Loading the CSV and Checking the Data"],"id":"s1f7JyBvYEpO"},{"cell_type":"code","metadata":{"id":"i_LKr5utYEpO"},"execution_count":null,"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv(train_csv, header=None)\n","test_df = pd.read_csv(test_csv, header=None)\n","\n","# CSV structure: [label, title, text]\n","train_df.columns = [\"label\", \"title\", \"text\"]\n","test_df.columns = [\"label\", \"title\", \"text\"]\n","\n","# Combine title + text into a single input sentence\n","train_df[\"text\"] = train_df[\"title\"] + \" \" + train_df[\"text\"]\n","test_df[\"text\"] = test_df[\"title\"] + \" \" + test_df[\"text\"]\n","\n","# Optionally remove unused columns\n","train_df = train_df[[\"label\", \"text\"]]\n","test_df = test_df[[\"label\", \"text\"]]\n","\n","print(\"Train size:\", len(train_df))\n","print(\"Test size:\", len(test_df))\n","train_df.head()"],"id":"i_LKr5utYEpO"},{"cell_type":"markdown","metadata":{"id":"dFm7voRiYEpO"},"source":["## 2. Tokenization & Vocabulary\n","\n","- We use a very simple **whitespace tokenizer**.\n","- Only words with frequency ‚â• `min_freq` are included in the vocab.\n","- We manually add the special tokens `<pad>` and `<unk>`.\n","\n","### üî∂ TODO 1\n","- Try changing `min_freq`, `max_vocab_size`, etc., and observe how they affect performance and speed."],"id":"dFm7voRiYEpO"},{"cell_type":"code","metadata":{"id":"hz0WOOa0YEpO"},"execution_count":null,"outputs":[],"source":["from collections import Counter\n","from typing import List\n","\n","PAD_TOKEN = \"<pad>\"\n","UNK_TOKEN = \"<unk>\"\n","\n","def tokenizer(text: str) -> List[str]:\n","    return text.lower().strip().split()\n","\n","def build_vocab(texts, min_freq: int = 5, max_vocab_size: int = 20000):\n","    counter = Counter()\n","    for t in texts:\n","        counter.update(tokenizer(t))\n","    # Sort by highest frequency\n","    most_common = counter.most_common(max_vocab_size)\n","    itos = [PAD_TOKEN, UNK_TOKEN]\n","    for word, freq in most_common:\n","        if freq >= min_freq:\n","            itos.append(word)\n","    stoi = {w: i for i, w in enumerate(itos)}\n","    return itos, stoi\n","\n","itos, stoi = build_vocab(train_df[\"text\"].tolist(), min_freq=5, max_vocab_size=20000)\n","vocab_size = len(itos)\n","\n","print(\"Vocab size:\", vocab_size)\n","print(\"First 10 tokens:\", itos[:10])"],"id":"hz0WOOa0YEpO"},{"cell_type":"markdown","metadata":{"id":"RBtjJUoBYEpP"},"source":["## 3. Dataset & DataLoader Definition\n","\n","- Convert text into sequences of integer IDs  \n","- Truncate or pad sequences to `max_len`  \n","- Convert labels into integers in the range 0~3  \n","\n","### Label Mapping\n","- **0 ‚Üí World**  \n","- **1 ‚Üí Sports**  \n","- **2 ‚Üí Business**  \n","- **3 ‚Üí Sci/Tech**\n","\n","### üî∂ TODO 2\n","- Try changing the `max_len` value and observe how it affects performance and training time."],"id":"RBtjJUoBYEpP"},{"cell_type":"code","metadata":{"id":"EHG2jbbTYEpP"},"execution_count":null,"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","def text_to_ids(text: str, stoi, max_len: int = 100):\n","    tokens = tokenizer(text)\n","    ids = [stoi.get(tok, stoi[UNK_TOKEN]) for tok in tokens]\n","    if len(ids) > max_len:\n","        ids = ids[:max_len]\n","    else:\n","        ids = ids + [stoi[PAD_TOKEN]] * (max_len - len(ids))\n","    return ids\n","\n","class NewsDataset(Dataset):\n","    def __init__(self, df, stoi, max_len=100):\n","        self.df = df.reset_index(drop=True)\n","        self.stoi = stoi\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        # original labels are 1~4, we convert to 0~3\n","        label = int(row[\"label\"]) - 1\n","        text = str(row[\"text\"])\n","        ids = text_to_ids(text, self.stoi, self.max_len)\n","        return torch.tensor(label, dtype=torch.long), torch.tensor(ids, dtype=torch.long)\n","\n","max_len = 100\n","batch_size = 128\n","\n","train_dataset = NewsDataset(train_df, stoi, max_len=max_len)\n","test_dataset = NewsDataset(test_df, stoi, max_len=max_len)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","labels, texts = next(iter(train_loader))\n","print(\"Batch labels shape:\", labels.shape)\n","print(\"Batch texts shape:\", texts.shape)"],"id":"EHG2jbbTYEpP"},{"cell_type":"markdown","metadata":{"id":"7zJVzHd6YEpP"},"source":["## 4. LSTM Text Classifier\n","\n","- Embedding layer  \n","- Bi-LSTM  \n","- Collect the final hidden states and feed them into a Linear layer for classification  \n","\n","We experiment with two versions of the embedding layer:  \n","1. Randomly initialized (baseline)  \n","2. Initialized with GloVe (Transfer Learning)"],"id":"7zJVzHd6YEpP"},{"cell_type":"code","metadata":{"id":"u1lhRyXSYEpP"},"execution_count":null,"outputs":[],"source":["import torch.nn as nn\n","\n","pad_idx = stoi[PAD_TOKEN]\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes,\n","                 num_layers=1, bidirectional=True, dropout=0.2,\n","                 embedding_matrix=None, freeze_embedding=False):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n","        if embedding_matrix is not None:\n","            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float))\n","        self.embedding.weight.requires_grad = not freeze_embedding\n","\n","        self.num_directions = 2 if bidirectional else 1\n","        self.lstm = nn.LSTM(\n","            input_size=embed_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            bidirectional=bidirectional,\n","            dropout=dropout if num_layers > 1 else 0.0,\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(hidden_dim * self.num_directions, num_classes)\n","\n","    def forward(self, x):\n","        emb = self.embedding(x)  # (B, L, E)\n","        output, (h_n, c_n) = self.lstm(emb)\n","        # ÎßàÏßÄÎßâ layerÏùò hidden stateÎßå ÏÇ¨Ïö©\n","        last_layer_h = h_n[-self.num_directions:, :, :]  # (num_directions, B, H)\n","        last_h = last_layer_h.transpose(0, 1).reshape(x.size(0), -1)\n","        out = self.fc(self.dropout(last_h))\n","        return out"],"id":"u1lhRyXSYEpP"},{"cell_type":"markdown","metadata":{"id":"FfXuslouYEpP"},"source":["## 5. Train / Evaluation Functions\n","\n","Feel free to modify the optimizer, scheduler, etc., if needed for your experiments."],"id":"FfXuslouYEpP"},{"cell_type":"code","metadata":{"id":"2MGktHl0YEpQ"},"execution_count":null,"outputs":[],"source":["import torch.optim as optim\n","\n","def train_one_epoch(model, loader, criterion, optimizer, device):\n","    model.train()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for labels, texts in loader:\n","        # Move data to device\n","        labels = labels.to(device)\n","        texts = texts.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(texts)\n","        loss = criterion(outputs, labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Accumulate loss and accuracy\n","        total_loss += loss.item() * labels.size(0)\n","        preds = outputs.argmax(dim=1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    return total_loss / total, correct / total\n","\n","\n","def evaluate(model, loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for labels, texts in loader:\n","            # Move data to device\n","            labels = labels.to(device)\n","            texts = texts.to(device)\n","\n","            outputs = model(texts)\n","            loss = criterion(outputs, labels)\n","\n","            # Accumulate loss and accuracy\n","            total_loss += loss.item() * labels.size(0)\n","            preds = outputs.argmax(dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    return total_loss / total, correct / total"],"id":"2MGktHl0YEpQ"},{"cell_type":"markdown","metadata":{"id":"GMY-1wW7YEpQ"},"source":["## 6. Experiment 1: Random Initialization (Baseline)\n","\n","### üî∂ TODO 3\n","- Run the cell below to evaluate the LSTM trained with **random embeddings**.\n","- Try adjusting `hidden_dim`, `num_layers`, and `num_epochs` to compare performance."],"id":"GMY-1wW7YEpQ"},{"cell_type":"code","metadata":{"id":"pV4YojzHYEpQ"},"execution_count":null,"outputs":[],"source":["vocab_size = len(itos)\n","embed_dim = 100\n","hidden_dim = 128\n","num_classes = 4\n","num_epochs = 5\n","lr = 1e-3\n","\n","baseline_model = LSTMClassifier(\n","    vocab_size=vocab_size,\n","    embed_dim=embed_dim,\n","    hidden_dim=hidden_dim,\n","    num_classes=num_classes,\n","    bidirectional=True,\n","    embedding_matrix=None,\n","    freeze_embedding=False,\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(baseline_model.parameters(), lr=lr)\n","\n","for epoch in range(1, num_epochs + 1):\n","    train_loss, train_acc = train_one_epoch(baseline_model, train_loader, criterion, optimizer, device)\n","    val_loss, val_acc = evaluate(baseline_model, test_loader, criterion, device)\n","    print(f\"[Baseline] Epoch {epoch}: \"\n","          f\"TrainLoss={train_loss:.4f} Acc={train_acc:.4f} | \"\n","          f\"TestLoss={val_loss:.4f} Acc={val_acc:.4f}\")"],"id":"pV4YojzHYEpQ"},{"cell_type":"markdown","metadata":{"id":"oQa2aR1YYEpQ"},"source":["## 7. Experiment 2: Pre-trained GloVe Initialization\n","\n","Now we initialize the embedding layer using **GloVe 6B 100d**.\n","\n","### 7-1. Downloading the GloVe File (only once)"],"id":"oQa2aR1YYEpQ"},{"cell_type":"code","metadata":{"id":"_SfNXnJ9YEpQ"},"execution_count":null,"outputs":[],"source":["glove_zip = \"glove.6B.zip\"\n","glove_txt = \"glove.6B.100d.txt\"\n","\n","if not os.path.exists(glove_txt):\n","    if not os.path.exists(glove_zip):\n","        !wget http://nlp.stanford.edu/data/glove.6B.zip\n","    !unzip -o glove.6B.zip\n","\n","print(\"GloVe file exists:\", os.path.exists(glove_txt))"],"id":"_SfNXnJ9YEpQ"},{"cell_type":"markdown","metadata":{"id":"n8pF_uUvYEpQ"},"source":["### 7-2. Loading GloVe and Building the Embedding Matrix\n","- Run the code below to create the `embedding_matrix`."],"id":"n8pF_uUvYEpQ"},{"cell_type":"code","metadata":{"id":"zJKH0BJXYEpQ"},"execution_count":null,"outputs":[],"source":["import numpy as np\n","\n","embedding_index = {}\n","with open(glove_txt, encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.rstrip().split(\" \")\n","        word = values[0]\n","        vector = np.asarray(values[1:], dtype=\"float32\")\n","        embedding_index[word] = vector\n","\n","print(\"GloVe vocab size:\", len(embedding_index))\n","\n","embedding_dim = 100\n","embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim)).astype(\"float32\")\n","\n","oov_count = 0\n","for idx, word in enumerate(itos):\n","    vec = embedding_index.get(word, None)\n","    if vec is not None:\n","        embedding_matrix[idx] = vec\n","    else:\n","        oov_count += 1\n","\n","print(\"OOV words:\", oov_count, \"/\", vocab_size)"],"id":"zJKH0BJXYEpQ"},{"cell_type":"markdown","metadata":{"id":"XvrGTw1rYEpR"},"source":["### 7-3. Training the LSTM Initialized with GloVe\n","\n","### üî∂ TODO 4\n","- Try both `freeze_embedding=True` and `freeze_embedding=False`.\n","- Compare how test accuracy and training speed differ between the two settings."],"id":"XvrGTw1rYEpR"},{"cell_type":"code","metadata":{"id":"BdfgvKSAYEpR"},"execution_count":null,"outputs":[],"source":["glove_model = LSTMClassifier(\n","    vocab_size=vocab_size,\n","    embed_dim=embedding_dim,\n","    hidden_dim=hidden_dim,\n","    num_classes=num_classes,\n","    bidirectional=True,\n","    embedding_matrix=embedding_matrix,\n","    freeze_embedding=False,  # TODO: also try setting this to True\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(glove_model.parameters(), lr=lr)\n","\n","for epoch in range(1, num_epochs + 1):\n","    train_loss, train_acc = train_one_epoch(glove_model, train_loader, criterion, optimizer, device)\n","    val_loss, val_acc = evaluate(glove_model, test_loader, criterion, device)\n","    print(f\"[GloVe Init] Epoch {epoch}: \"\n","          f\"TrainLoss={train_loss:.4f} Acc={train_acc:.4f} | \"\n","          f\"TestLoss={val_loss:.4f} Acc={val_acc:.4f}\")"],"id":"BdfgvKSAYEpR"},{"cell_type":"markdown","metadata":{"id":"APM4KTMzYEpR"},"source":["## 8. Analysis (Report)\n","\n","üî∂ Write brief analyses for the following:\n","\n","1. **Test Accuracy Comparison**  \n","   - Random init vs. GloVe (freeze / non-freeze)\n","\n","2. **Training**  \n","   - Discuss which setting learns better/faster.\n","\n","3. **Effect of Pre-trained Embeddings**  \n","   - Briefly describe whether pre-training appears to act as a form of regularization.\n","\n","4. **Embedding Analysis**  \n","   - Compare the learned embeddings from Random init vs. GloVe init.\n","   - You may examine word similarities, PCA/TSNE visualizations, OOV handling differences, etc."],"id":"APM4KTMzYEpR"}]}